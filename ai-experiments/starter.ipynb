{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
    "llm = OpenAI()\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Translate html to text\n",
    "import html2text\n",
    "import os\n",
    "\n",
    "def extract_text(html_file_path):\n",
    "    html = open(html_file_path).read()\n",
    "    return html2text.html2text(html)\n",
    "\n",
    "def get_transformed_files():\n",
    "    file_paths = [f\"./data/{file}\" for file in os.listdir('./data')]\n",
    "\n",
    "    out_file_paths = []\n",
    "    for file_path in file_paths:\n",
    "        if file_path.endswith('.html'):\n",
    "            text = extract_text(file_path)\n",
    "            with open(file_path.replace('.html', '.txt'), 'w') as f:\n",
    "                f.write(text)\n",
    "                out_file_paths.append(file_path.replace('.html', '.txt'))\n",
    "        else:\n",
    "            out_file_paths.append(file_path)\n",
    "    return list(set(out_file_paths))\n",
    "\n",
    "file_paths = get_transformed_files()\n",
    "documents = SimpleDirectoryReader(input_files=file_paths).load_data()\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author wrote short stories and tried to program on an IBM 1401.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='The author wrote short stories and tried to program on an IBM 1401.', source_nodes=[NodeWithScore(node=TextNode(id_='925e09e8-15b3-43f1-84e5-6e1d49286f0d', embedding=None, metadata={'file_path': 'data/local_example_tutorial.txt', 'file_name': 'local_example_tutorial.txt', 'file_type': 'text/plain', 'file_size': 100832, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7c1853f6-a307-49fa-b79f-bc7aa30095e7', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': 'data/local_example_tutorial.txt', 'file_name': 'local_example_tutorial.txt', 'file_type': 'text/plain', 'file_size': 100832, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, hash='5c9bd3e48ad66ffb4a2b5837b83312a18f372c033616a08f083313c15ef07a75'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='6fa1048f-7cc5-431f-8871-ae102275bd40', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': 'data/local_example_tutorial.txt', 'file_name': 'local_example_tutorial.txt', 'file_type': 'text/plain', 'file_size': 100832, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, hash='6c47400628ce2360629e48f01c9f3abf1f93ba6217858fae26bf514f1db28088'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='5a5b67f4-55a4-4091-a799-69ab31736601', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='02d7ef7991a87830d2a1d9b3043f28aa689503b8d136546485fdf6cc08011c03')}, text='Your directory structure should look like this:\\n\\n    \\n    \\n    ├── starter.py\\n    └── data\\n        └── paul_graham_essay.txt\\n    \\n\\nWe use the `BAAI/bge-small-en-v1.5` model through `resolve_embed_model`, which\\nresolves to our HuggingFaceEmbedding class. We also use our `Ollama` LLM\\nwrapper to load in the mistral model.\\n\\n## Query your data#\\n\\nAdd the following lines to `starter.py`\\n\\n    \\n    \\n    query_engine = index.as_query_engine()\\n    response = query_engine.query(\"What did the author do growing up?\")\\n    print(response)\\n    \\n\\nThis creates an engine for Q&A over your index and asks a simple question. You\\nshould get back a response similar to the following: `The author wrote short\\nstories and tried to program on an IBM 1401.`\\n\\nYou can view logs, persist/load the index similar to our [starter\\nexample](../starter_example/).\\n\\nTip\\n\\n  * learn more about the [high-level concepts](../concepts/).\\n  * tell me how to [customize things](../customization/).\\n  * curious about a specific module? check out the [component guides](../../module_guides/).\\n\\nBack to top  [ Previous  Starter Tutorial (OpenAI)  ](../starter_example/) [\\nNext  Discover LlamaIndex Video Series  ](../discover_llamaindex/)', start_char_idx=99593, end_char_idx=100791, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.2749715129213797)], metadata={'925e09e8-15b3-43f1-84e5-6e1d49286f0d': {'file_path': 'data/local_example_tutorial.txt', 'file_name': 'local_example_tutorial.txt', 'file_type': 'text/plain', 'file_size': 100832, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "def ask(question):\n",
    "    response = query_engine.query(question)\n",
    "    print(response)\n",
    "    return response\n",
    "\n",
    "# ask(\"What did the author do growing up?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Source: Documentation)\n",
      "The Discord link is: [https://discord.gg/Ekmcn2f3](https://discord.gg/Ekmcn2f3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='(Source: Documentation)\\nThe Discord link is: [https://discord.gg/Ekmcn2f3](https://discord.gg/Ekmcn2f3)', sources=[ToolOutput(content='[https://discord.gg/Ekmcn2f3](https://discord.gg/Ekmcn2f3)', tool_name='query_engine_tool', raw_input={'input': 'Discord link'}, raw_output=Response(response='[https://discord.gg/Ekmcn2f3](https://discord.gg/Ekmcn2f3)', source_nodes=[NodeWithScore(node=TextNode(id_='58faabb4-2061-4783-9a82-40b6b8eefb73', embedding=None, metadata={'file_path': 'data/hackathon_index.txt', 'file_name': 'hackathon_index.txt', 'file_type': 'text/plain', 'file_size': 20067, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6cdff03b-1c18-4f95-a32e-c152e54c601e', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': 'data/hackathon_index.txt', 'file_name': 'hackathon_index.txt', 'file_type': 'text/plain', 'file_size': 20067, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, hash='415c394714e3cef2afe575340b873c1a3acc68964e641336e8d3b0e24bc3a020'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a48f6fc8-7e9e-4168-a50d-07cdf620ec90', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': 'data/hackathon_index.txt', 'file_name': 'hackathon_index.txt', 'file_type': 'text/plain', 'file_size': 20067, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, hash='c6a005c2f6b8f4e6863523b5d1febc6355847e36572b768ee3938e0f3f122ce1'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='9a6a05e8-3076-4e17-9779-022b74b9a013', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='89acbd171ee292c520c9f9c8b720af0cf7cc59c062176136e36bc182b7cff506')}, text='[Facebook\\nIcon](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)![Facebook\\nIcon](/developer/facebook.svg)](https://www.facebook.com/sharer.php?u= \"Share\\non\\nFacebook\")[![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2712%27%20height=%2712%27/%3e)![twitter\\nicon](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)![twitter\\nicon](/developer/twitter.svg)](https://twitter.com/intent/tweet?url=&text=MongoDB%20GenAI%20Hackathon%20SF%20%23MongoDB\\n\"Share on\\nTwitter\")[![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2712%27%20height=%2712%27/%3e)![linkedin\\nicon](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)![linkedin\\nicon](/developer/linkedin.svg)](https://www.linkedin.com/shareArticle?mini=true&url=&title=MongoDB\\nGenAI Hackathon SF&summary=MongoDB GenAI Hackathon SF&source=MongoDB \"Share on\\nLinkedIn\")\\n\\nIndustry Event | In-Person\\n\\ncalendar\\n\\n###### When\\n\\nApr 20, 2024 - Apr 21, 2024\\n\\nlocation\\n\\n###### Location\\n\\nSan Francisco, CA, USA\\n\\n[More Info](https://lu.ma/MongoDB-GenAI-SF)\\n\\n![feature_image](/developer/_next/image/?url=https%3A%2F%2Fimages.contentstack.io%2Fv3%2Fassets%2Fblt39790b633ee0d5a7%2Fbltbe0c26c8129301a6%2F6621a60ec9de46f9dad45b7a%2FScreenshot_2024-04-18_at_4.00.13_PM.png&w=3840&q=75)\\n\\nWelcome to the MongoDB GenAI Hackathon! Below are all the resources you will\\nneed for the day.\\n\\n## Discord Channel\\n\\n[https://discord.gg/Ekmcn2f3](https://discord.gg/Ekmcn2f3)\\n\\n## MongoDB\\n\\nMongoDB is renowned in the industry as a top-tier operational database. With\\n[MongoDB Atlas](https://www.mongodb.com/lp/cloud/atlas/), we have unified\\nMongoDB’s operational and vector database capabilities into a single unified\\nplatform. With Atlas, you can combine the benefits of a flexible document\\nmodel and an extensive aggregation framework with vector search to build\\npowerful AI applications for various use cases. As enterprises go from RAG to\\nagents, MongoDB Atlas can also serve as long and short-term memory for agents.\\n\\n**Lightning Talk**\\n\\n  * [Llama Index and RAG Getting Started](https://github.com/mongodb-developer/GenAI-Showcase/blob/main/notebooks/rag/building_RAG_with_LlamaIndex_and_MongoDB_Vector_Database.ipynb) by [John Willis](https://www.linkedin.com/in/johnwillisatlanta/), Author & Consultant, MongoDB\\n\\n**Quickstarts**\\n\\n  * [Getting started with MongoDB Atlas and Python](https://github.com/mongodb-developer/mongodb-atlas-python-quickstart/blob/main/quickstart-1-getting-started-atlas-python.ipynb)\\n\\n  * [How to Perform Semantic Search on Your Data Using MongoDB Atlas](https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/vector-search-tutorial/#std-label-vector-search-tutorial)\\n\\n**Credits**\\n\\nTo get $100 Atlas credits, do the following:\\n\\n  * Sign-up for or login to your [MongoDB Atlas account](http://account.mongodb.com)\\n\\n  * Click Billing at the top of your [MongoDB Atlas account page](https://account.mongodb.com/).\\n\\n  * Enter the code provided on-site into the “Apply Code” box at the bottom of the MongoDB Atlas billing page.', start_char_idx=9475, end_char_idx=12638, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.30709702905733716)], metadata={'58faabb4-2061-4783-9a82-40b6b8eefb73': {'file_path': 'data/hackathon_index.txt', 'file_name': 'hackathon_index.txt', 'file_type': 'text/plain', 'file_size': 20067, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}}), is_error=False)], source_nodes=[NodeWithScore(node=TextNode(id_='58faabb4-2061-4783-9a82-40b6b8eefb73', embedding=None, metadata={'file_path': 'data/hackathon_index.txt', 'file_name': 'hackathon_index.txt', 'file_type': 'text/plain', 'file_size': 20067, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6cdff03b-1c18-4f95-a32e-c152e54c601e', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': 'data/hackathon_index.txt', 'file_name': 'hackathon_index.txt', 'file_type': 'text/plain', 'file_size': 20067, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, hash='415c394714e3cef2afe575340b873c1a3acc68964e641336e8d3b0e24bc3a020'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a48f6fc8-7e9e-4168-a50d-07cdf620ec90', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': 'data/hackathon_index.txt', 'file_name': 'hackathon_index.txt', 'file_type': 'text/plain', 'file_size': 20067, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, hash='c6a005c2f6b8f4e6863523b5d1febc6355847e36572b768ee3938e0f3f122ce1'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='9a6a05e8-3076-4e17-9779-022b74b9a013', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='89acbd171ee292c520c9f9c8b720af0cf7cc59c062176136e36bc182b7cff506')}, text='[Facebook\\nIcon](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)![Facebook\\nIcon](/developer/facebook.svg)](https://www.facebook.com/sharer.php?u= \"Share\\non\\nFacebook\")[![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2712%27%20height=%2712%27/%3e)![twitter\\nicon](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)![twitter\\nicon](/developer/twitter.svg)](https://twitter.com/intent/tweet?url=&text=MongoDB%20GenAI%20Hackathon%20SF%20%23MongoDB\\n\"Share on\\nTwitter\")[![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2712%27%20height=%2712%27/%3e)![linkedin\\nicon](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)![linkedin\\nicon](/developer/linkedin.svg)](https://www.linkedin.com/shareArticle?mini=true&url=&title=MongoDB\\nGenAI Hackathon SF&summary=MongoDB GenAI Hackathon SF&source=MongoDB \"Share on\\nLinkedIn\")\\n\\nIndustry Event | In-Person\\n\\ncalendar\\n\\n###### When\\n\\nApr 20, 2024 - Apr 21, 2024\\n\\nlocation\\n\\n###### Location\\n\\nSan Francisco, CA, USA\\n\\n[More Info](https://lu.ma/MongoDB-GenAI-SF)\\n\\n![feature_image](/developer/_next/image/?url=https%3A%2F%2Fimages.contentstack.io%2Fv3%2Fassets%2Fblt39790b633ee0d5a7%2Fbltbe0c26c8129301a6%2F6621a60ec9de46f9dad45b7a%2FScreenshot_2024-04-18_at_4.00.13_PM.png&w=3840&q=75)\\n\\nWelcome to the MongoDB GenAI Hackathon! Below are all the resources you will\\nneed for the day.\\n\\n## Discord Channel\\n\\n[https://discord.gg/Ekmcn2f3](https://discord.gg/Ekmcn2f3)\\n\\n## MongoDB\\n\\nMongoDB is renowned in the industry as a top-tier operational database. With\\n[MongoDB Atlas](https://www.mongodb.com/lp/cloud/atlas/), we have unified\\nMongoDB’s operational and vector database capabilities into a single unified\\nplatform. With Atlas, you can combine the benefits of a flexible document\\nmodel and an extensive aggregation framework with vector search to build\\npowerful AI applications for various use cases. As enterprises go from RAG to\\nagents, MongoDB Atlas can also serve as long and short-term memory for agents.\\n\\n**Lightning Talk**\\n\\n  * [Llama Index and RAG Getting Started](https://github.com/mongodb-developer/GenAI-Showcase/blob/main/notebooks/rag/building_RAG_with_LlamaIndex_and_MongoDB_Vector_Database.ipynb) by [John Willis](https://www.linkedin.com/in/johnwillisatlanta/), Author & Consultant, MongoDB\\n\\n**Quickstarts**\\n\\n  * [Getting started with MongoDB Atlas and Python](https://github.com/mongodb-developer/mongodb-atlas-python-quickstart/blob/main/quickstart-1-getting-started-atlas-python.ipynb)\\n\\n  * [How to Perform Semantic Search on Your Data Using MongoDB Atlas](https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/vector-search-tutorial/#std-label-vector-search-tutorial)\\n\\n**Credits**\\n\\nTo get $100 Atlas credits, do the following:\\n\\n  * Sign-up for or login to your [MongoDB Atlas account](http://account.mongodb.com)\\n\\n  * Click Billing at the top of your [MongoDB Atlas account page](https://account.mongodb.com/).\\n\\n  * Enter the code provided on-site into the “Apply Code” box at the bottom of the MongoDB Atlas billing page.', start_char_idx=9475, end_char_idx=12638, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.30709702905733716)], is_dummy_stream=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine = index.as_chat_engine()\n",
    "chat_engine.chat(\"\"\"\n",
    "Your goal is to help me finding things on the documentation I provided. I want your answers to be in this format\n",
    "(Source: <source>)\n",
    "<answer>\n",
    "\n",
    "source: in best case scenario, a URL, or the file name\n",
    "answer: the answer to the question\n",
    "\"\"\")\n",
    "def chat(question):\n",
    "    response = chat_engine.chat(question)\n",
    "    print(response)\n",
    "    return response\n",
    "\n",
    "chat(\"What's the discord link?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Source: Documentation)\n",
      "The previous question was \"What did the author do growing up?\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='(Source: Documentation)\\nThe previous question was \"What did the author do growing up?\"', sources=[ToolOutput(content='The previous question was \"What did the author do growing up?\"', tool_name='query_engine_tool', raw_input={'input': 'What was my previous question?'}, raw_output=Response(response='The previous question was \"What did the author do growing up?\"', source_nodes=[NodeWithScore(node=TextNode(id_='925e09e8-15b3-43f1-84e5-6e1d49286f0d', embedding=None, metadata={'file_path': 'data/local_example_tutorial.txt', 'file_name': 'local_example_tutorial.txt', 'file_type': 'text/plain', 'file_size': 100832, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7c1853f6-a307-49fa-b79f-bc7aa30095e7', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': 'data/local_example_tutorial.txt', 'file_name': 'local_example_tutorial.txt', 'file_type': 'text/plain', 'file_size': 100832, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, hash='5c9bd3e48ad66ffb4a2b5837b83312a18f372c033616a08f083313c15ef07a75'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='6fa1048f-7cc5-431f-8871-ae102275bd40', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': 'data/local_example_tutorial.txt', 'file_name': 'local_example_tutorial.txt', 'file_type': 'text/plain', 'file_size': 100832, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, hash='6c47400628ce2360629e48f01c9f3abf1f93ba6217858fae26bf514f1db28088'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='5a5b67f4-55a4-4091-a799-69ab31736601', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='02d7ef7991a87830d2a1d9b3043f28aa689503b8d136546485fdf6cc08011c03')}, text='Your directory structure should look like this:\\n\\n    \\n    \\n    ├── starter.py\\n    └── data\\n        └── paul_graham_essay.txt\\n    \\n\\nWe use the `BAAI/bge-small-en-v1.5` model through `resolve_embed_model`, which\\nresolves to our HuggingFaceEmbedding class. We also use our `Ollama` LLM\\nwrapper to load in the mistral model.\\n\\n## Query your data#\\n\\nAdd the following lines to `starter.py`\\n\\n    \\n    \\n    query_engine = index.as_query_engine()\\n    response = query_engine.query(\"What did the author do growing up?\")\\n    print(response)\\n    \\n\\nThis creates an engine for Q&A over your index and asks a simple question. You\\nshould get back a response similar to the following: `The author wrote short\\nstories and tried to program on an IBM 1401.`\\n\\nYou can view logs, persist/load the index similar to our [starter\\nexample](../starter_example/).\\n\\nTip\\n\\n  * learn more about the [high-level concepts](../concepts/).\\n  * tell me how to [customize things](../customization/).\\n  * curious about a specific module? check out the [component guides](../../module_guides/).\\n\\nBack to top  [ Previous  Starter Tutorial (OpenAI)  ](../starter_example/) [\\nNext  Discover LlamaIndex Video Series  ](../discover_llamaindex/)', start_char_idx=99593, end_char_idx=100791, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.23342787991229338)], metadata={'925e09e8-15b3-43f1-84e5-6e1d49286f0d': {'file_path': 'data/local_example_tutorial.txt', 'file_name': 'local_example_tutorial.txt', 'file_type': 'text/plain', 'file_size': 100832, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}}), is_error=False)], source_nodes=[NodeWithScore(node=TextNode(id_='925e09e8-15b3-43f1-84e5-6e1d49286f0d', embedding=None, metadata={'file_path': 'data/local_example_tutorial.txt', 'file_name': 'local_example_tutorial.txt', 'file_type': 'text/plain', 'file_size': 100832, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7c1853f6-a307-49fa-b79f-bc7aa30095e7', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': 'data/local_example_tutorial.txt', 'file_name': 'local_example_tutorial.txt', 'file_type': 'text/plain', 'file_size': 100832, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, hash='5c9bd3e48ad66ffb4a2b5837b83312a18f372c033616a08f083313c15ef07a75'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='6fa1048f-7cc5-431f-8871-ae102275bd40', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': 'data/local_example_tutorial.txt', 'file_name': 'local_example_tutorial.txt', 'file_type': 'text/plain', 'file_size': 100832, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, hash='6c47400628ce2360629e48f01c9f3abf1f93ba6217858fae26bf514f1db28088'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='5a5b67f4-55a4-4091-a799-69ab31736601', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='02d7ef7991a87830d2a1d9b3043f28aa689503b8d136546485fdf6cc08011c03')}, text='Your directory structure should look like this:\\n\\n    \\n    \\n    ├── starter.py\\n    └── data\\n        └── paul_graham_essay.txt\\n    \\n\\nWe use the `BAAI/bge-small-en-v1.5` model through `resolve_embed_model`, which\\nresolves to our HuggingFaceEmbedding class. We also use our `Ollama` LLM\\nwrapper to load in the mistral model.\\n\\n## Query your data#\\n\\nAdd the following lines to `starter.py`\\n\\n    \\n    \\n    query_engine = index.as_query_engine()\\n    response = query_engine.query(\"What did the author do growing up?\")\\n    print(response)\\n    \\n\\nThis creates an engine for Q&A over your index and asks a simple question. You\\nshould get back a response similar to the following: `The author wrote short\\nstories and tried to program on an IBM 1401.`\\n\\nYou can view logs, persist/load the index similar to our [starter\\nexample](../starter_example/).\\n\\nTip\\n\\n  * learn more about the [high-level concepts](../concepts/).\\n  * tell me how to [customize things](../customization/).\\n  * curious about a specific module? check out the [component guides](../../module_guides/).\\n\\nBack to top  [ Previous  Starter Tutorial (OpenAI)  ](../starter_example/) [\\nNext  Discover LlamaIndex Video Series  ](../discover_llamaindex/)', start_char_idx=99593, end_char_idx=100791, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.23342787991229338)], is_dummy_stream=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"what was my previous question?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To set the OpenAI Embed model into Llama Index, you need to ensure that you have the OPENAI_API_KEY set up as an environment variable. This key can be obtained by logging into your OpenAI account and creating a new API key. Once you have the API key, you can integrate the OpenAI `text-embedding-ada-002` model with Llama Index for text retrieval and embeddings.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='To set the OpenAI Embed model into Llama Index, you need to ensure that you have the OPENAI_API_KEY set up as an environment variable. This key can be obtained by logging into your OpenAI account and creating a new API key. Once you have the API key, you can integrate the OpenAI `text-embedding-ada-002` model with Llama Index for text retrieval and embeddings.', sources=[ToolOutput(content='To set the OpenAI Embed model into Llama Index, you need to ensure that you have the OPENAI_API_KEY set up as an environment variable. This key can be obtained by logging into your OpenAI account and creating a new API key. Once you have the API key, you can integrate the OpenAI `text-embedding-ada-002` model with Llama Index for text retrieval and embeddings.', tool_name='query_engine_tool', raw_input={'input': 'Set the OpenAI Embed model into Llama Index'}, raw_output=Response(response='To set the OpenAI Embed model into Llama Index, you need to ensure that you have the OPENAI_API_KEY set up as an environment variable. This key can be obtained by logging into your OpenAI account and creating a new API key. Once you have the API key, you can integrate the OpenAI `text-embedding-ada-002` model with Llama Index for text retrieval and embeddings.', source_nodes=[NodeWithScore(node=TextNode(id_='abf93524-ee11-4b9a-a7e3-26aae1bbd557', embedding=None, metadata={'file_path': 'data/llama_index_installation.txt', 'file_name': 'llama_index_installation.txt', 'file_type': 'text/plain', 'file_size': 101152, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='bfe84f5f-3021-4ca1-9f90-a3d525f422a6', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': 'data/llama_index_installation.txt', 'file_name': 'llama_index_installation.txt', 'file_type': 'text/plain', 'file_size': 101152, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, hash='3c5e96cbbef125d93ba6265fa12f71e52720df4f4fff5db5a66049d694a23e23'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a9c5ac17-8af5-4cab-841e-db4e5f59ca5f', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': 'data/llama_index_installation.txt', 'file_name': 'llama_index_installation.txt', 'file_type': 'text/plain', 'file_size': 101152, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, hash='a02a2fc3cd9111410c09acacbd56c550ab33e4aec738efa8b6ca5beaeff4bea9'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='bca334ce-ac2a-4228-93fc-2ceefe7a531a', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='1036c8df2f248d16448dc9febdf1ad56cd577233bdb50ad25d7340648b7e671c')}, text=\"### Important: OpenAI Environment Setup#\\n\\nBy default, we use the OpenAI `gpt-3.5-turbo` model for text generation and\\n`text-embedding-ada-002` for retrieval and embeddings. In order to use this,\\nyou must have an OPENAI_API_KEY set up as an environment variable. You can\\nobtain an API key by logging into your OpenAI account and [and creating a new\\nAPI key](https://platform.openai.com/account/api-keys).\\n\\nTip\\n\\nYou can also [use one of many other available\\nLLMs](../../module_guides/models/llms/usage_custom/). You may need additional\\nenvironment keys + tokens setup depending on the LLM provider.\\n\\n[Check out our OpenAI Starter Example](../starter_example/)\\n\\n## Custom Installation from Pip#\\n\\nIf you aren't using OpenAI, or want a more selective installation, you can\\ninstall individual packages as needed.\\n\\nFor example, for a local setup with Ollama and HuggingFace embeddings, the\\ninstallation might look like:\\n\\n    \\n    \\n    pip install llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-huggingface\\n    \\n\\n[Check out our Starter Example with Local Models](../starter_example_local/)\\n\\nA full guide to using and configuring LLMs is available\\n[here](../../module_guides/models/llms/).\\n\\nA full guide to using and configuring embedding models is available\\n[here](../../module_guides/models/embeddings/).\\n\\n## Installation from Source#\\n\\nGit clone this repository: `git clone\\nhttps://github.com/jerryjliu/llama_index.git`. Then do the following:\\n\\n  * [Install poetry](https://python-poetry.org/docs/#installation) \\\\- this will help you manage package dependencies\\n  * `poetry shell` \\\\- this command creates a virtual environment, which keeps installed packages contained to this project\\n  * `poetry install` \\\\- this will install the core starter package requirements \\n  * (Optional) `poetry install --with dev, docs` \\\\- this will install all dependencies needed for most local development\\n\\nFrom there, you can install integrations as needed with `pip`, For example:\\n\\n    \\n    \\n    pip install -e llama-index-integrations/llms/llama-index-llms-ollama\\n    \\n\\nBack to top  [ Previous  High-Level Concepts  ](../concepts/) [ Next  How to\\nread these docs  ](../reading/)\", start_char_idx=98932, end_char_idx=101129, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5158913482426025)], metadata={'abf93524-ee11-4b9a-a7e3-26aae1bbd557': {'file_path': 'data/llama_index_installation.txt', 'file_name': 'llama_index_installation.txt', 'file_type': 'text/plain', 'file_size': 101152, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}}), is_error=False)], source_nodes=[NodeWithScore(node=TextNode(id_='abf93524-ee11-4b9a-a7e3-26aae1bbd557', embedding=None, metadata={'file_path': 'data/llama_index_installation.txt', 'file_name': 'llama_index_installation.txt', 'file_type': 'text/plain', 'file_size': 101152, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='bfe84f5f-3021-4ca1-9f90-a3d525f422a6', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': 'data/llama_index_installation.txt', 'file_name': 'llama_index_installation.txt', 'file_type': 'text/plain', 'file_size': 101152, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, hash='3c5e96cbbef125d93ba6265fa12f71e52720df4f4fff5db5a66049d694a23e23'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a9c5ac17-8af5-4cab-841e-db4e5f59ca5f', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': 'data/llama_index_installation.txt', 'file_name': 'llama_index_installation.txt', 'file_type': 'text/plain', 'file_size': 101152, 'creation_date': '2024-04-20', 'last_modified_date': '2024-04-20'}, hash='a02a2fc3cd9111410c09acacbd56c550ab33e4aec738efa8b6ca5beaeff4bea9'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='bca334ce-ac2a-4228-93fc-2ceefe7a531a', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='1036c8df2f248d16448dc9febdf1ad56cd577233bdb50ad25d7340648b7e671c')}, text=\"### Important: OpenAI Environment Setup#\\n\\nBy default, we use the OpenAI `gpt-3.5-turbo` model for text generation and\\n`text-embedding-ada-002` for retrieval and embeddings. In order to use this,\\nyou must have an OPENAI_API_KEY set up as an environment variable. You can\\nobtain an API key by logging into your OpenAI account and [and creating a new\\nAPI key](https://platform.openai.com/account/api-keys).\\n\\nTip\\n\\nYou can also [use one of many other available\\nLLMs](../../module_guides/models/llms/usage_custom/). You may need additional\\nenvironment keys + tokens setup depending on the LLM provider.\\n\\n[Check out our OpenAI Starter Example](../starter_example/)\\n\\n## Custom Installation from Pip#\\n\\nIf you aren't using OpenAI, or want a more selective installation, you can\\ninstall individual packages as needed.\\n\\nFor example, for a local setup with Ollama and HuggingFace embeddings, the\\ninstallation might look like:\\n\\n    \\n    \\n    pip install llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-huggingface\\n    \\n\\n[Check out our Starter Example with Local Models](../starter_example_local/)\\n\\nA full guide to using and configuring LLMs is available\\n[here](../../module_guides/models/llms/).\\n\\nA full guide to using and configuring embedding models is available\\n[here](../../module_guides/models/embeddings/).\\n\\n## Installation from Source#\\n\\nGit clone this repository: `git clone\\nhttps://github.com/jerryjliu/llama_index.git`. Then do the following:\\n\\n  * [Install poetry](https://python-poetry.org/docs/#installation) \\\\- this will help you manage package dependencies\\n  * `poetry shell` \\\\- this command creates a virtual environment, which keeps installed packages contained to this project\\n  * `poetry install` \\\\- this will install the core starter package requirements \\n  * (Optional) `poetry install --with dev, docs` \\\\- this will install all dependencies needed for most local development\\n\\nFrom there, you can install integrations as needed with `pip`, For example:\\n\\n    \\n    \\n    pip install -e llama-index-integrations/llms/llama-index-llms-ollama\\n    \\n\\nBack to top  [ Previous  High-Level Concepts  ](../concepts/) [ Next  How to\\nread these docs  ](../reading/)\", start_char_idx=98932, end_char_idx=101129, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5158913482426025)], is_dummy_stream=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"How do I set the openai embed model into llama index?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mongo-hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
